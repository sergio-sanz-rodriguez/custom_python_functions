{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating syntetic database can be use to train machine learning\n",
    "\n",
    "\"\"\"\n",
    "For binary classification.\n",
    "\n",
    "The dataset will have the following specifications:\n",
    "\n",
    "Task: Binary Classification\n",
    "Features: 10 numerical features, generated based on a mix of distributions to simulate real-world data complexity\n",
    "Samples: 1000 data points\n",
    "Labels: 0 or 1, with approximately equal distribution\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Settings\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "n_classes = 2\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_features-2, n_redundant=0, n_classes=n_classes, flip_y=0.01, class_sep=1.5, random_state=42)\n",
    "\n",
    "# Convert to pandas DataFrame for ease of use\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_features)])\n",
    "df['label'] = y\n",
    "\n",
    "# Save dataset to CSV file (optional)\n",
    "df.to_csv('synthetic_classification_dataset.csv', index=False)\n",
    "\n",
    "# Print the first few rows to check\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For gradient problems.\n",
    "\n",
    "Let's generate a synthetic dataset with the following specifications:\n",
    "\n",
    "Task: Regression\n",
    "Features: 10 numerical features, generated to simulate real-world scenarios with a mix of linear and non-linear relationships.\n",
    "Samples: 1000 data points\n",
    "Target: A continuous value derived from the features plus some noise to simulate real-world data variance.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Settings\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "noise_level = 0.1\n",
    "\n",
    "# Generate synthetic features\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate a target variable with a mix of linear and non-linear relationships\n",
    "# Example: target = 3*feature_1 + 2*np.sin(feature_2) - log(1 + feature_3) + noise\n",
    "noise = np.random.normal(0, noise_level, n_samples)\n",
    "target = 3*X[:, 0] + 2*np.sin(X[:, 1]) - np.log(1 + np.abs(X[:, 2])) + noise\n",
    "\n",
    "# Convert to pandas DataFrame for ease of use\n",
    "features_df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(n_features)])\n",
    "target_df = pd.DataFrame(target, columns=['target'])\n",
    "\n",
    "# Combine features and target into one DataFrame\n",
    "df = pd.concat([features_df, target_df], axis=1)\n",
    "\n",
    "# Save dataset to CSV file (optional)\n",
    "df.to_csv('synthetic_regression_dataset.csv', index=False)\n",
    "\n",
    "# Print the first few rows to check\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembel\n",
    "\n",
    "Ensemble methods are techniques in machine learning that combine the predictions from multiple models to improve robustness, reduce variance, and increase accuracy compared to any single model used within the ensemble. These methods are particularly useful for complex prediction tasks where it's challenging to achieve high performance with a single model. Let's delve into some of the most common ensemble methods: bagging, boosting, stacking, and blending, along with an overview of others.\n",
    "\n",
    "### 1. Bagging (Bootstrap Aggregating)\n",
    "- **Principle:** Bagging involves training multiple models of the same type on different subsets of the training data. These subsets are created by randomly sampling the original dataset with replacement (bootstrap samples). The final prediction is typically the average of the predictions (for regression tasks) or the majority vote (for classification tasks).\n",
    "- **Example:** Random Forest is a popular bagging-based ensemble method that uses multiple decision trees.\n",
    "\n",
    "### 2. Boosting\n",
    "- **Principle:** Boosting is a sequential process where each model attempts to correct the errors of the previous models. The data points that were misclassified or had a higher error by previous models are given more weight, so subsequent models focus more on difficult cases. This process continues until a specified number of models are created or no further improvements can be made.\n",
    "- **Examples:** AdaBoost (Adaptive Boosting) and Gradient Boosting are well-known boosting methods. XGBoost, LightGBM, and CatBoost are advanced implementations that are highly popular in machine learning competitions due to their performance and speed.\n",
    "\n",
    "### 3. Stacking (Stacked Generalization)\n",
    "- **Principle:** Stacking involves training a new model (meta-learner or blender) to combine the predictions of several different models. The original models are trained on the full dataset, and then their predictions are used as inputs to train the meta-learner to produce the final prediction. This method leverages the strength of each base model and reduces bias and variance.\n",
    "- **Example:** The base level can include diverse models like decision trees, SVMs, and neural networks, while the meta-learner could be a logistic regression model.\n",
    "\n",
    "### 4. Blending\n",
    "- **Principle:** Blending is similar to stacking but with a slight difference in how the training set for the meta-learner is created. Instead of using out-of-fold predictions from the base models (as in stacking), blending uses a holdout set (a validation set that is not part of the cross-validation used to train the base models) to train the meta-learner.\n",
    "- **Example:** If the dataset is split into 80% training and 20% test data, the training data might be further split, with 70% used to train the base models and 10% as a holdout set for the blender.\n",
    "\n",
    "### Other Ensemble Methods\n",
    "- **Voting:** Simplest form of ensemble, where the predictions from multiple models are combined through a majority vote (for classification) or average (for regression).\n",
    "- **Snapshot Ensembling:** Involves saving snapshots of a single model at different epochs during training, particularly when the model's performance on a validation set improves. These snapshots are then averaged to make the final prediction.\n",
    "- **Model Averaging:** A straightforward approach where the final prediction is the average of the predictions from multiple models. It's a simple but often effective method, especially when the models are diverse.\n",
    "\n",
    "Ensemble methods can significantly improve prediction performance by combining the strengths and reducing the weaknesses of individual models. However, they also tend to increase computational complexity and training time. It's essential to balance performance improvements with computational costs, especially in real-world applications where resources might be limited.\n",
    "\n",
    "[sklearn](https://scikit-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie plot for target\n",
    "\n",
    "colors = ['gold', 'mediumturquoise']\n",
    "labels = ['0','1']\n",
    "values = df['Outcome'].value_counts()/df['Outcome'].shape[0]\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
    "fig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,\n",
    "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
    "fig.update_layout(\n",
    "    title_text=\"Outcome\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [cname for cname in df.loc[:,:'Age'].columns]\n",
    "\n",
    "# Histogram\n",
    "rcParams['figure.figsize'] = 40,60\n",
    "sns.set(font_scale = 3)\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"bright\")\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "i = 1;\n",
    "for name in feature_names:\n",
    "    plt.subplot(5,2,i)\n",
    "    sns.histplot(data=df, x=name, hue=\"Outcome\",kde=True,palette=\"YlGnBu\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram 2\n",
    "\n",
    "# 'bin_edges' is a list of bin intervals\n",
    "count, bin_edges = np.histogram(df['2013'])\n",
    "\n",
    "df['2013'].plot(kind='hist', figsize=(8, 5), xticks=bin_edges)\n",
    "\n",
    "plt.title('Histogram of Immigration from 195 countries in 2013') # add a title to the histogram\n",
    "plt.ylabel('Number of Countries') # add y-label\n",
    "plt.xlabel('Number of Immigrants') # add x-label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "sns.set(font_scale=2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"bright\")\n",
    "sns.pairplot(df,kind = 'reg',corner = True,palette ='YlGnBu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "fig = px.histogram(df, x=\"Glucose\", \n",
    "                   color=\"Outcome\", \n",
    "                   marginal=\"box\",\n",
    "                   barmode =\"overlay\",\n",
    "                   histnorm ='density'\n",
    "                  )  \n",
    "fig.update_layout(\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    title={\n",
    "        'text': \"Glucose Histogram per Outcome\",\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing relevant feature\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Assuming 'X_transformed' is the output from your preprocessor and 'y' is your target variable\n",
    "# Note: Adjust this example if you're using the pipeline approach to directly fit on raw data\n",
    "\n",
    "# Fit SelectKBest\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # 'all' to keep all features for demonstration\n",
    "X_new = selector.fit(X_transformed, y_train)\n",
    "\n",
    "# Get scores and p-values\n",
    "scores = selector.scores_\n",
    "p_values = selector.pvalues_\n",
    "\n",
    "# Otherwise, if you're directly applying SelectKBest after manual preprocessing:\n",
    "feature_names = ['age', 'pregnancies', 'bmi', 'skinthickness', 'insulin', 'glucose',\n",
    "       'bloodpressure', 'diabetespedigreefunction']  # Fill this with your actual feature names\n",
    "\n",
    "# Create a DataFrame to display scores and p-values\n",
    "import pandas as pd\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'ANOVA F-Score': scores,\n",
    "    'p-value': p_values\n",
    "}).sort_values(by='ANOVA F-Score', ascending=False)\n",
    "\n",
    "print(feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corplot\n",
    "corr=df.corr().round(2)\n",
    "\n",
    "sns.set_theme(font_scale=1.15)\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set_palette(\"bright\")\n",
    "sns.set_style(\"white\")\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr,annot=True,cmap='gist_yarg_r',mask=mask,cbar=True)\n",
    "plt.title('Correlation Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing threshold and calculate different validation scores\n",
    "\n",
    "def calculator (y_test, y_predict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Optimizing final stimator\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, GradientBoostingClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#Optimizing final estimator\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Load some example data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=10, random_state=42)),\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Construct stacking model\n",
    "stack_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Prepare parameter grid, targeting meta-model\n",
    "param_grid = {\n",
    "    'final_estimator__C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'final_estimator__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Instantiate and run GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=stack_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to store evaluation and plot them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate temperature by day\n",
    "daily_data_df = data_df \\\n",
    "    .groupby(['date', 'year', 'month', 'day', 'dayofyear'], as_index=False)\\\n",
    "    .agg({'temperature': np.mean}) \\\n",
    "    .set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waffle Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pywaffle\n",
    "#!pip install pywaffle\n",
    "from pywaffle import Waffle\n",
    "\n",
    "#Set up the Waffle chart figure\n",
    "\n",
    "fig = plt.figure(FigureClass = Waffle,\n",
    "                 rows = 20, columns = 30, #pass the number of rows and columns for the waffle \n",
    "                 values = df_dsn['Total'], #pass the data to be used for display\n",
    "                 cmap_name = 'tab20', #color scheme\n",
    "                 legend = {'labels': [f\"{k} ({v})\" for k, v in zip(df_dsn.index.values,df_dsn.Total)],\n",
    "                            'loc': 'lower left', 'bbox_to_anchor':(0,-0.1),'ncol': 3}\n",
    "                 #notice the use of list comprehension for creating labels \n",
    "                 #from index and total of the dataset\n",
    "                )\n",
    "\n",
    "#Display the waffle chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force way\n",
    "\n",
    "def create_waffle_chart(categories, values, height, width, colormap, value_sign=''):\n",
    "\n",
    "    # compute the proportion of each category with respect to the total\n",
    "    total_values = sum(values)\n",
    "    category_proportions = [(float(value) / total_values) for value in values]\n",
    "\n",
    "    # compute the total number of tiles\n",
    "    total_num_tiles = width * height # total number of tiles\n",
    "    print ('Total number of tiles is', total_num_tiles)\n",
    "    \n",
    "    # compute the number of tiles for each catagory\n",
    "    tiles_per_category = [round(proportion * total_num_tiles) for proportion in category_proportions]\n",
    "\n",
    "    # print out number of tiles per category\n",
    "    for i, tiles in enumerate(tiles_per_category):\n",
    "        print (df_dsn.index.values[i] + ': ' + str(tiles))\n",
    "    \n",
    "    # initialize the waffle chart as an empty matrix\n",
    "    waffle_chart = np.zeros((height, width))\n",
    "\n",
    "    # define indices to loop through waffle chart\n",
    "    category_index = 0\n",
    "    tile_index = 0\n",
    "\n",
    "    # populate the waffle chart\n",
    "    for col in range(width):\n",
    "        for row in range(height):\n",
    "            tile_index += 1\n",
    "\n",
    "            # if the number of tiles populated for the current category \n",
    "            # is equal to its corresponding allocated tiles...\n",
    "            if tile_index > sum(tiles_per_category[0:category_index]):\n",
    "                # ...proceed to the next category\n",
    "                category_index += 1       \n",
    "            \n",
    "            # set the class value to an integer, which increases with class\n",
    "            waffle_chart[row, col] = category_index\n",
    "    \n",
    "    # instantiate a new figure object\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # use matshow to display the waffle chart\n",
    "    colormap = plt.cm.coolwarm\n",
    "    plt.matshow(waffle_chart, cmap=colormap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # get the axis\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # set minor ticks\n",
    "    ax.set_xticks(np.arange(-.5, (width), 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, (height), 1), minor=True)\n",
    "    \n",
    "    # add dridlines based on minor ticks\n",
    "    ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # compute cumulative sum of individual categories to match color schemes between chart and legend\n",
    "    values_cumsum = np.cumsum(values)\n",
    "    total_values = values_cumsum[len(values_cumsum) - 1]\n",
    "\n",
    "    # create legend\n",
    "    legend_handles = []\n",
    "    for i, category in enumerate(categories):\n",
    "        if value_sign == '%':\n",
    "            label_str = category + ' (' + str(values[i]) + value_sign + ')'\n",
    "        else:\n",
    "            label_str = category + ' (' + value_sign + str(values[i]) + ')'\n",
    "            \n",
    "        color_val = colormap(float(values_cumsum[i])/total_values)\n",
    "        legend_handles.append(mpatches.Patch(color=color_val, label=label_str))\n",
    "\n",
    "    # add legend to chart\n",
    "    plt.legend(\n",
    "        handles=legend_handles,\n",
    "        loc='lower center', \n",
    "        ncol=len(categories),\n",
    "        bbox_to_anchor=(0., -0.2, 0.95, .1)\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "width = 40 # width of chart\n",
    "height = 10 # height of chart\n",
    "\n",
    "categories = df_dsn.index.values # categories\n",
    "values = df_dsn['Total'] # correponding values of categories\n",
    "\n",
    "colormap = plt.cm.coolwarm # color map class\n",
    "\n",
    "create_waffle_chart(categories, values, height, width, colormap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
